{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LinearRegression.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_xrOayznOUg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from numpy.linalg import inv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1B88kB7aP3FC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def hypothesis(X, coeff):\n",
        "  return np.dot(X, coeff)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUeI9M2l_MIl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Feature scaling\n",
        "def mean(a):\n",
        "  mean = float(np.mean(a))\n",
        "  func = np.vectorize(lambda t: t - mean)\n",
        "  return func(a)\n",
        "\n",
        "def std_var(a):\n",
        "  std = float(np.std(a))\n",
        "  func = np.vectorize(lambda t: t / std)\n",
        "  return func(a)\n",
        "\n",
        "def feature_scaling(X):\n",
        "  X = np.apply_along_axis(mean, 0, X)\n",
        "  X = np.apply_along_axis(std_var, 0, X)\n",
        "  return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGKFEVWoEJRB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalizeXy(X, y):\n",
        "#   Change as per notation to get: w0x0 + w1x1 + w2x2 + ...\n",
        "# i.e. add an extra feature vector x0 for bias\n",
        "# Also do feature scaling\n",
        "#   assert X.shape == (303, 13)\n",
        "  \n",
        "  shape = (X.shape[0], 1)\n",
        "#   assert shape == (303, 1)\n",
        "  \n",
        "  newX = feature_scaling(X)\n",
        "  newX = np.hstack((np.ones(shape), newX))\n",
        "  \n",
        "#   assert newX.shape == (303, 14)\n",
        "  \n",
        "  newy = y.reshape((-1,1))\n",
        "#   assert newy.shape == (303, 1)\n",
        "  \n",
        "  return newX, newy\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5ch5qMmIz1u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Method 1 to find theta: Direct Normal Equation\n",
        "\n",
        "def findTheta(X, y):\n",
        "#   Instead of using gradient descent to estimate theta, use normal equation to get theta\n",
        "# theta = ((Xt.X)^-1).Xt.y\n",
        "  pinv = inv(np.dot(X.T, X))\n",
        "  nextDot = np.dot(pinv, X.T)\n",
        "  return np.dot(nextDot, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_EugirpyWsY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Method 2 to find theta: Gradient descent\n",
        "\n",
        "# loss: (303, 1)\n",
        "# X: (303, 14)\n",
        "\n",
        "def hypothesis(X, coeff):\n",
        "  return np.dot(X, coeff)\n",
        "\n",
        "def gradient(X, y, theta):\n",
        "  m = float(X.shape[0])\n",
        "  hx = hypothesis(X, theta) \n",
        "  loss = hx - y\n",
        "  grad =  1/m * np.dot(X.T, loss)\n",
        "  return grad \n",
        "\n",
        "def cost_function(X, y, theta): \n",
        "  m = float(X.shape[0])\n",
        "  hx = hypothesis(X, theta) \n",
        "  loss = hx - y\n",
        "  cost = (1/2*m) * np.sum(np.square(loss))\n",
        "  return cost\n",
        "\n",
        "def gradient_descent(X, y, coeff, learning_rate = 0.01, min_cost_change = 0.0001, max_epochs = 50000):\n",
        "  epoch = 1 \n",
        "  current_cost = cost_function(X, y, coeff) \n",
        "  cost_change = learning_rate\n",
        "  \n",
        "#   for epoch in range(0,1000): \n",
        "  while cost_change > min_cost_change and epoch < max_epochs:\n",
        "    prev_cost = current_cost\n",
        "    grad = gradient(X, y, coeff)\n",
        "    coeff = coeff - (learning_rate * grad) \n",
        "    current_cost = cost_function(X, y, coeff) \n",
        "    cost_change = prev_cost - current_cost\n",
        "#     print(\"cost_change:\", cost_change)\n",
        "    epoch += 1\n",
        "  \n",
        "  return coeff, epoch "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1htQY8chqCT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Method 2 to find theta: Mini batch gradient descent\n",
        "# def calc_cost(X, y, theta): \n",
        "#   m = float(X.shape[0])\n",
        "#   hx = hypothesis(X, theta) \n",
        "#   cost = (1/2*m) * np.sum(np.square(hx - y))\n",
        "  \n",
        "# def create_mini_batches(X, y, batch_size): \n",
        "#     mini_batches = []\n",
        "#     data = np.hstack((X, y)) \n",
        "#     np.random.shuffle(data) \n",
        "#     n_minibatches = data.shape[0] // batch_size \n",
        "#     i = 0\n",
        "  \n",
        "#     for i in range(n_minibatches + 1): \n",
        "#       if data.shape[0] % batch_size != 0: \n",
        "#         mini_batch = data[i * batch_size:data.shape[0]]\n",
        "#       else:\n",
        "#         mini_batch = data[i * batch_size:(i + 1)*batch_size, :] \n",
        "      \n",
        "#       X_mini = mini_batch[:, :-1] \n",
        "#       Y_mini = mini_batch[:, -1].reshape((-1, 1)) \n",
        "#       mini_batches.append((X_mini, Y_mini))\n",
        "#     return mini_batches \n",
        "  \n",
        "# def mini_batch_gradient_descent(X, y, learning_rate = 0.001, batch_size = 32): \n",
        "#     theta = np.zeros((X.shape[1], 1)) \n",
        "#     cost_history = [] \n",
        "#     max_iters = 3\n",
        "#     for itr in range(max_iters): \n",
        "#         mini_batches = create_mini_batches(X, y, batch_size) \n",
        "#         for mini_batch in mini_batches: \n",
        "#             X_mini, y_mini = mini_batch \n",
        "#             theta = theta - learning_rate * gradient(X_mini, y_mini, theta) \n",
        "#             cost_history.append(calc_cost(X_mini, y_mini, theta)) \n",
        "  \n",
        "#     return theta, cost_history "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJ-bteQOPWBb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_y(X, coeff): \n",
        "    pred_value = hypothesis(X, coeff) \n",
        "    return pred_value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t71J4-TjUoSR",
        "colab_type": "code",
        "outputId": "bf22b013-3312-4043-a166-e8404e831300",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  dataset = datasets.load_boston()\n",
        "  X = dataset.data\n",
        "  y = dataset.target\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "  print(\"X_train.shape: \", X_train.shape)\n",
        "  print(\"y_train.shape: \", y_train.shape)\n",
        "  \n",
        "  X_train, y_train = normalizeXy(X_train, y_train)\n",
        "  print(\"New X_train.shape after normalization: \", X_train.shape)\n",
        "  print(\"New y_train.shape after normalization: \", y_train.shape)\n",
        "  \n",
        "#   coeff = findTheta(X_train, y_train)\n",
        "\n",
        "#   Initial values\n",
        "  coeff = np.random.rand(X_train.shape[1], 1)\n",
        "  coeff, epoch = gradient_descent(X_train, y_train, coeff)\n",
        "  print(\"\\ncoeff.shape: \", coeff.shape)\n",
        "  print(\"coefficients: \", coeff)\n",
        "  print(\"epoch: \",epoch)\n",
        "  \n",
        "\n",
        "  y_pred = predict_y(X_train, coeff)\n",
        "#   assert(y_pred.shape == (303,1))\n",
        "  \n",
        "  print(\"\\ny actual vs predicted:\")\n",
        "  print(y_train[:3])\n",
        "  print(y_pred[:3])\n",
        "  \n",
        "  rmse = np.sqrt(metrics.mean_squared_error(y_train, y_pred))\n",
        "  print(\"\\nrmse: \", format(rmse, '.5g'))\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train.shape:  (404, 13)\n",
            "y_train.shape:  (404,)\n",
            "New X_train.shape after normalization:  (404, 14)\n",
            "New y_train.shape after normalization:  (404, 1)\n",
            "\n",
            "coeff.shape:  (14, 1)\n",
            "coefficients:  [[22.5730198 ]\n",
            " [-0.77618933]\n",
            " [ 0.81012723]\n",
            " [ 0.14594059]\n",
            " [ 0.25528977]\n",
            " [-1.88666879]\n",
            " [ 3.18390405]\n",
            " [-0.14780517]\n",
            " [-2.72987965]\n",
            " [ 2.20934025]\n",
            " [-2.00794581]\n",
            " [-2.3085894 ]\n",
            " [ 0.78881118]\n",
            " [-3.03760937]]\n",
            "epoch:  9895\n",
            "\n",
            "y actual vs predicted:\n",
            "[[23. ]\n",
            " [19.6]\n",
            " [22. ]]\n",
            "[[19.6613528 ]\n",
            " [18.33184451]\n",
            " [29.22369433]]\n",
            "\n",
            "rmse:  4.5176\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBpofaljAbtt",
        "colab_type": "code",
        "outputId": "20130b14-e089-4917-de8e-399b4389648e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Using scikit libraries:\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "X2_train, X2_test, y2_train, y2_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X2_train = scaler.fit_transform(X2_train)\n",
        "\n",
        "reg = LinearRegression().fit(X2_train, y2_train)\n",
        "y2_pred = reg.predict(X2_train)\n",
        "\n",
        "print(\"\\ny actual vs predicted:\")\n",
        "print(y2_train[:3])\n",
        "print(y2_pred[:3])\n",
        "  \n",
        "rmse = np.sqrt(metrics.mean_squared_error(y2_train, y2_pred))\n",
        "print(\"\\nrmse: \", format(rmse, '.5g'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "y actual vs predicted:\n",
            "[16.1 13.1 20.3]\n",
            "[19.13827971 14.42493049 19.71903769]\n",
            "\n",
            "rmse:  4.7684\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cc9ljxdGg4lB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}