{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SGDbasic.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGDv8xg0t4YP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# gradient: del f(x,y) - vector of partial derivatives; direction of steepest ascent\n",
        "# an optimization algorithm to find the minimum of a function - gradient desccent \n",
        "# gd - move in the negative direction of the gradient of the function to reach the local/global minima.\n",
        "# gd with a batch of 1 - sgd\n",
        "# Every iter like:\n",
        "#     Xc = Xp - lr * dy/dx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akqt8hZPuEbr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Say loss function y = (x+5)^2\n",
        "learning_rate = 0.01\n",
        "max_epochs = 1000\n",
        "starting_point_x = 3\n",
        "# Naive gradient. Can use a sigmoid activation function to get a gradient as in Logistic assignement.\n",
        "gradient = lambda x: 2 * (x+5) \n",
        "\n",
        "previous_pos_x = 1\n",
        "epoch = 1\n",
        "min_step = 0.0001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8l2bCgUluLc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "current_pos_x = starting_point_x\n",
        "step_size = learning_rate\n",
        "while (step_size > min_step and epoch < max_epochs):\n",
        "    print(\"Epoch no: \", epoch)\n",
        "    prev_pos_x = current_pos_x\n",
        "    current_pos_x = current_pos_x - learning_rate * gradient(prev_pos_x)\n",
        "    step_size = abs(current_pos_x - prev_pos_x)\n",
        "    print(\"Value of x: \",current_pos_x, \", loss: \", step_size)\n",
        "    epoch += 1\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNvrDej3uMDd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# As seen, local minima is -5 and the algo stops at ~ -4.99, loss keeps decreasing"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}